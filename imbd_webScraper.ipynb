{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imbd_webScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZcbM3D08FctkSXmkml76O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiherreira/webscraperIMDb/blob/main/imbd_webScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-X2HVAxlO7b"
      },
      "source": [
        "## The IMDb Web Scraper\n",
        "\n",
        "**Addapted by**: [Willian Herreira Lima](https://www.linkedin.com/in/willianherreira/).\n",
        "\n",
        "To build this model I did follow the step-by-step guide written by [Angeliga Dietzel](https://medium.com/better-programming/the-only-step-by-step-guide-youll-need-to-build-a-web-scraper-with-python-e79066bd895a) and published on medium.com. The entire [template code](https://github.com/angelicadietzel/data-projects/blob/master/single-page-web-scraper/imdb_scraper.py) is available at Angeliga's data-project folder.  \n",
        "\n",
        "What is web scraping? Web scraping consists of gathering data available on websites to build your own databases, it is a powerful tool to have on your portfolio. Some scrapers can be started manually or be automatically triggered by a function call.\n",
        "\n",
        "The article is really interesting, it covers topics such as understanding HTML web pages, building a web scraper using Python, and creating a DataFrame with pandas. It also covers data quality, data cleaning, and data-type conversion, it is an entirely step-by-step and with instructions, code, and explanations on how every piece of it works. I did add a few twists to my model, first I did add the “Director” to the data storage, and a nested “for loop” to build a list of urls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGrV3fclNcw"
      },
      "source": [
        "# Import Libraries \n",
        "import requests\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNqpMDzrMQd"
      },
      "source": [
        "## Look for patterns\n",
        "\n",
        "An important aspect of system automation is to look for patterns, note that the first fifth titles' URL differs from the following pages after the and (\"&\") character, also note that the only difference between further addresses is where to start the query by item number on the web site's list. Check the addresses listed below to spot these patterns:\n",
        "\n",
        "* 1-50 of 1,000 titles.\n",
        "> https://www.imdb.com/search/title/?groups=top_1000&ref_=adv_prv\n",
        "\n",
        "* Next » 51-100 of 1,000 titles\n",
        "> https://www.imdb.com/search/title/?groups=top_1000&start=51&ref_=adv_nxt\n",
        "\n",
        "* Next » 101-150 of 1,000 titles\n",
        "> https://www.imdb.com/search/title/?groups=top_1000&start=101&ref_=adv_nxt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09uAz6_wKWo6"
      },
      "source": [
        "As a side note, if you run the code from a country where English is not the main language, it’s very likely that you’ll get some of the movie names translated into the main language of that country. Most likely, this happens because the server infers your location from your IP address. Even if you are located in a country where English is the main language, you may still get translated content. This may happen if you’re using a VPN while you’re making the GET requests. For example, as I'm in Australia connected to a VPN somewhere in Asia, the scraper it first output was in, what I believe to be, hanzi (Chinese characters ).\n",
        "\n",
        "If you run into this issue, pass the following values to the headers parameter of the get() function:\n",
        "\n",
        "**headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27k7D5x4vGH3"
      },
      "source": [
        "## Create Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV2QB3fmvPfg"
      },
      "source": [
        "# Get a list of all urls to the IMDb \"Top 1000\"\n",
        "#urls = [\"https://www.imdb.com/search/title/?groups=top_1000&ref_=adv_prv\", \n",
        "#        \"https://www.imdb.com/search/title/?groups=top_1000&start=51&ref_=adv_nxt\",\n",
        "#        \"https://www.imdb.com/search/title/?groups=top_1000&start=101&ref_=adv_nxt\"]\n",
        "\n",
        "urls=[\"https://www.imdb.com/search/title/?groups=top_1000&ref_=adv_prv\"]\n",
        "url_fp = \"https://www.imdb.com/search/title/?groups=top_1000&start=\"\n",
        "url_sp = \"&ref_=adv_nxt\"\n",
        "for ii in range(0,1000,50):\n",
        "  if ii%50 == 0:\n",
        "    if ii > 1:\n",
        "      url = url_fp+str(ii+1)+url_sp\n",
        "      urls.append(url)\n",
        "\n",
        "# Initiate data storage\n",
        "titles = []\n",
        "years = []\n",
        "time = []\n",
        "imdb_ratings = []\n",
        "metascores = []\n",
        "votes = []\n",
        "us_gross = []\n",
        "#Need to figure how to get Director name\n",
        "directors = []\n",
        "\n",
        "# A nexted loop for each url.\n",
        "for url in urls:\n",
        "  headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
        "  results = requests.get(url, headers=headers)\n",
        "  #Beautiful Soup is a Python library for pulling data out of HTML and XML files. \n",
        "  soup = BeautifulSoup(results.text, \"html.parser\")\n",
        "  movie_div = soup.find_all('div',class_='lister-item mode-advanced')\n",
        "\n",
        "  #this tells your scraper to iterate through \n",
        "  #every div container we stored in move_div\n",
        "  #\n",
        "  # for each lister-item mode-advanced div container:\n",
        "  #     scrape these elements\n",
        "  \n",
        "  for container in movie_div:\n",
        "    # Name\n",
        "    titles.append(container.h3.a.text)\n",
        "  \n",
        "    #year\n",
        "    years.append(container.h3.find('span', class_='lister-item-year').text)\n",
        "  \n",
        "    #runtime\n",
        "    runtime = container.p.find('span', class_='runtime').text if container.p.find('span', class_='runtime').text else '-'\n",
        "    time.append(runtime)\n",
        "  \n",
        "    #IMDb Rating\n",
        "    imdb_ratings.append(float(container.strong.text))\n",
        "  \n",
        "    #Metascore\n",
        "    # Some movies might not have a Metascore, to avoide a ValueError performing\n",
        "    # the data cleaning we added \"-1\" which can be consireded false or NULL. \n",
        "    scores = container.find('span', class_='metascore').text if container.find('span', class_='metascore') else -1\n",
        "    metascores.append(scores)\n",
        "    # Votes\n",
        "    nv = container.find_all('span', attrs={'name':'nv'})\n",
        "    votes.append(nv[0].text) # filter number of votes\n",
        "    # Revenue\n",
        "    grosses =nv[1].text if len(nv) > 1 else '-'\n",
        "    us_gross.append(grosses)\n",
        "    #Director\n",
        "    director = container.find_all('p',class_='')\n",
        "    director = director[1].find('a').text\n",
        "    directors.append(director)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_rfapZLTK_3"
      },
      "source": [
        "## Build a DataFrame using Pandas\n",
        "\n",
        "Next we use Pandas to join the lists created and clean the data. For example, if we print the first five \"years\" the output will look similar to this:\n",
        "\n",
        ">> ['(2020)', '(2020)', '(1978)', '(1993)', '(1940)']\n",
        "\n",
        "Therefore, we used regex to extract only digits from a string and then have it converted to a integer. This method is slightly changed when cleaning the revenue data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50WrcUdqegl"
      },
      "source": [
        "# Joining lists into a dictionary\n",
        "movies = pd.DataFrame({\n",
        "    'movie': titles,\n",
        "    'year': years,\n",
        "    'timeMin': time,\n",
        "    'imdb': imdb_ratings,\n",
        "    'metascores': metascores,\n",
        "    'votes': votes,\n",
        "    'director': directors,\n",
        "    'us_grossMillions': us_gross,\n",
        "})\n",
        "\n",
        "#Clean Data\n",
        "movies['year'] = movies['year'].str.extract('(\\d+)').astype(int)\n",
        "movies['timeMin'] = movies['timeMin'].str.extract('(\\d+)').astype(int)\n",
        "movies['metascores'] = movies['metascores'].astype(int)\n",
        "movies['votes'] = movies['votes'].str.replace(',','').astype(int)\n",
        "movies['us_grossMillions'] = movies['us_grossMillions'].map(lambda x: x.lstrip('$').rstrip('M'))\n",
        "movies['us_grossMillions'] = pd.to_numeric(movies['us_grossMillions'], errors='coerce')\n",
        "\n",
        "#add dataframe to csv name 'movies.csv'\n",
        "movies.to_csv('movies.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ingKyKZa7e"
      },
      "source": [
        "## SAVE FILE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5CBXB4JT-I8",
        "outputId": "d9c1ee5a-6dfb-4afd-dd18-cc9508f9f593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# Save csv file to future use.\n",
        "files.download('movies.csv')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ef802207-1b71-46e9-897b-96ecc71f139b\", \"movies.csv\", 62260)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}